import os, json, cv2, boto3, traceback, time, threading, math, re
from datetime import datetime, timezone
from sqlalchemy import create_engine, text
from dotenv import load_dotenv
import logging
from collections import defaultdict

# Device policy: respect env; set FORCE_CPU=1 to hard-force CPU
if os.getenv("FORCE_CPU","0") == "1":
    os.environ.pop("CUDA_VISIBLE_DEVICES", None)
    os.environ["ULTRALYTICS_DEVICE"] = "cpu"

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s [worker_video] %(message)s")

# Load shared API .env so worker sees the same knobs
load_dotenv(os.path.join(os.path.dirname(__file__), "..", "api", ".env"))

AWS_REGION             = os.getenv("AWS_REGION", "ap-southeast-2")
S3_BUCKET              = os.getenv("S3_BUCKET") or os.getenv("BUCKET")
SQS_VIDEO_QUEUE_URL    = os.getenv("SQS_VIDEO_QUEUE_URL")
SQS_SNAPSHOT_QUEUE_URL = os.getenv("SQS_SNAPSHOT_QUEUE_URL")
DB_URL                 = os.getenv("DB_URL")
FRAME_STRIDE           = int(os.getenv("FRAME_STRIDE", "3"))

# Progress/format knobs
PROG_INT   = int(os.getenv("PROGRESS_LOG_INTERVAL_SEC", "10"))
PROG_JSON  = bool(int(os.getenv("PROGRESS_LOG_JSON", "0")))

# (Optional) SQS visibility heartbeat knobs for long videos
VIS_HEARTBEAT_SEC = int(os.getenv("SQS_VIS_HEARTBEAT_SEC", "0"))     # 0 disables
VIS_TIMEOUT       = int(os.getenv("SQS_VIS_TIMEOUT", "300"))

# DB url normalize
if DB_URL and DB_URL.startswith("postgresql+psycopg2://"):
    DB_URL = DB_URL.replace("postgresql+psycopg2://","postgresql://",1)

sqs = boto3.client("sqs", region_name=AWS_REGION)
s3  = boto3.client("s3",  region_name=AWS_REGION)
engine = create_engine(DB_URL, pool_pre_ping=True, future=True) if DB_URL else None

# Imports from our modular components
from workers.yolo import YoloDetector, CLASS_NAMES
from workers.tracker import DeepSortTracker
from workers.snapshotper import (
    BestFrameKeeper, build_snapshot_key, save_and_upload_snapshot, choose_margin_for_neighbors
)

# Time helpers (existing module)
from workers.timecode import ms_from_frame, iso_add_ms

def utcnow_iso(): return datetime.now(timezone.utc).isoformat()

def _emit_existing_snapshots(payload: dict):
    """
    List s3://{S3_BUCKET}/demo_user/{workspace_id}/{video_id}/snapshots/
    and emit one SNAPSHOT_READY per object (helper used in some backfills).
    """
    bucket = S3_BUCKET
    if not bucket: return
    ws_id = str(payload.get("workspace_id","")); vid = str(payload.get("video_id",""))
    ws_code = str(payload.get("workspace_code","")); cam_code_hint = str(payload.get("camera_code",""))
    recorded_at = payload.get("recordedAt")
    prefix = f"demo_user/{ws_id}/{vid}/snapshots/"
    paginator = s3.get_paginator("list_objects_v2")
    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get("Contents", []):
            key = obj.get("Key","")
            m = re.search(r'/([^/_]+)_([^/_]+)_(\d{6})_(\d{6})\.jpg$', key)
            track_id = int(m.group(3)) if m else 0
            offset_ms = int(m.group(4)) if m else None
            cam_code = cam_code_hint or (m.group(2) if m else "")
            msg = {
                "event": "SNAPSHOT_READY",
                "video_id": vid,
                "workspace_id": ws_id,
                "workspace_code": ws_code,
                "camera_code": cam_code,
                "track_id": track_id,
                "snapshot_s3_key": key,
                "recordedAt": recorded_at,
                "detectedIn": offset_ms,
                "detectedAt": iso_add_ms(recorded_at, offset_ms) if (recorded_at and offset_ms is not None) else None,
                "yolo_type": "VEHICLE"
            }
            emit_snapshot_ready(msg)

def set_status(video_id: str, status: str, err: str | None = None):
    if not engine: return
    with engine.begin() as conn:
        if err:
            conn.execute(
                text("UPDATE videos SET status=:s, updated_at=NOW(), error_msg=:e WHERE id=:id"),
                {"s": status, "e": err[:500], "id": video_id},
            )
        else:
            conn.execute(
                text("UPDATE videos SET status=:s, updated_at=NOW(), error_msg=NULL WHERE id=:id"),
                {"s": status, "id": video_id},
            )

def download_to_tmp(s3_key: str, path: str):
    s3.download_file(S3_BUCKET, s3_key, path)
    return path

def emit_snapshot_ready(payload: dict):
    sqs.send_message(QueueUrl=SQS_SNAPSHOT_QUEUE_URL, MessageBody=json.dumps(payload))

def _log_progress(pass_id:int, payload:dict):
    if PROG_JSON:
        logging.info("[progress] " + json.dumps({"pass":pass_id, **payload}))
    else:
        parts = [f"[progress] pass={pass_id}"] + [f"{k}={v}" for k,v in payload.items()]
        logging.info(" ".join(parts))

def handle_process_video(body: dict):
    vid         = body["video_id"]
    ws_id       = body["workspace_id"]
    ws_code     = body["workspace_code"]
    cam_code    = body["camera_code"]
    s3_key_raw  = body["s3_key_raw"]
    frame_stride= int(body.get("frame_stride", FRAME_STRIDE))
    recorded_at = body.get("recordedAt")

    set_status(vid, "processing")

    local = f"/tmp/{vid}.mp4"
    download_to_tmp(s3_key_raw, local)

    det = YoloDetector()
    trk = DeepSortTracker()

    cap = cv2.VideoCapture(local)
    fps = cap.get(cv2.CAP_PROP_FPS) or 0.0
    if fps <= 0: fps = 25.0
    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
    iters_total = int(math.ceil((total_frames or 0) / max(1, frame_stride))) if total_frames else 0

    best = BestFrameKeeper(W, H)
    frame_to_boxes: dict[int, list[tuple[int, tuple[int,int,int,int]]]] = defaultdict(list)

    # -------- Pass 1: detect/track + pick best frames --------
    frame_idx = -1
    last_log = time.time()
    t_start = last_log
    iters = 0
    last_det_count = 0

    while True:
        ok, frame = cap.read()
        if not ok: break
        frame_idx += 1
        if frame_idx % frame_stride != 0: continue

        iters += 1
        dets = det.infer(frame)
        last_det_count = len(dets)

        # DeepSORT expects [x1,y1,x2,y2,conf,cls] â†’ our adapter already handles dets format
        tracks = trk.update(dets, frame)

        # record boxes for neighbor suppression later (pass 2)
        for t in tracks:
            if not isinstance(t, dict) or "tlbr" not in t: continue
            tlbr = tuple(map(int, t["tlbr"]))
            frame_to_boxes[frame_idx].append((int(t["id"]), tlbr))

        # feed BestFrameKeeper
        for t in tracks:
            best.consider(tid=t["id"], frame_idx=frame_idx, bbox=t["tlbr"], conf=t["conf"], cls_id=t["cls"])

        now = time.time()
        if now - last_log >= PROG_INT:
            elapsed = now - t_start
            iter_sec = round(iters / elapsed, 2) if elapsed > 0 else 0.0
            fps_approx = round((iters * frame_stride) / elapsed, 2) if elapsed > 0 else 0.0
            eta = 0
            if iters_total and iter_sec > 0:
                eta = int(round((iters_total - iters) / max(iter_sec, 1e-6)))
            _log_progress(1, dict(iters=iters, iters_total=iters_total, iter_sec=iter_sec,
                                  fps_approx=fps_approx, dets_last=last_det_count, ETA=f"{eta}s"))
            last_log = now

    cap.release()

    # -------- Pass 2: emit snapshots with neighbor-aware margin --------
    ready = best.items_ready()
    total_ready = len(ready)
    if total_ready == 0:
        set_status(vid, "done")
        logging.info("No tracks ready; done VID=%s", vid)
        return

    cap = cv2.VideoCapture(local)
    emit_count = 0
    t2_start = time.time()
    last_log2 = t2_start

    for tid, rec in ready:
        cap.set(cv2.CAP_PROP_POS_FRAMES, rec.frame_idx)
        ok, frame = cap.read()
        if not ok:
            continue

        # Neighbor-aware margin
        neighbors = [tlbr for (other_tid, tlbr) in frame_to_boxes.get(rec.frame_idx, []) if other_tid != tid]
        margin = choose_margin_for_neighbors(rec.bbox, neighbors, W, H)

        offset_ms = ms_from_frame(rec.frame_idx, fps)
        key_rel = build_snapshot_key(
            user_id="demo_user",
            workspace_id=ws_id, video_id=vid,
            workspace_code=ws_code, camera_code=cam_code,
            track_id=tid, offset_ms=offset_ms
        )

        # Use chosen margin
        save_and_upload_snapshot(frame, rec.bbox, s3, S3_BUCKET, key_rel, margin=margin)

        snapshot_uri = f"s3://{S3_BUCKET}/{key_rel}"
        yolo_type = CLASS_NAMES[rec.cls_id] if 0 <= rec.cls_id < len(CLASS_NAMES) else "VEHICLE"

        emit_snapshot_ready({
            "event": "SNAPSHOT_READY",
            "video_id": vid,
            "workspace_id": ws_id,
            "workspace_code": ws_code,
            "camera_code": cam_code,
            "track_id": tid,
            "snapshot_s3_key": snapshot_uri,
            "recordedAt": recorded_at,
            "detectedIn": offset_ms,
            "detectedAt": iso_add_ms(recorded_at, offset_ms) if recorded_at else None,
            "yolo_type": yolo_type
        })
        emit_count += 1

        now = time.time()
        if now - last_log2 >= PROG_INT:
            elapsed = now - t2_start
            snaps_sec = round(emit_count / elapsed, 2) if elapsed > 0 else 0.0
            _log_progress(2, dict(emitted=emit_count, total=total_ready, snapshots_sec=snaps_sec,
                                  elapsed=f"{int(elapsed)}s"))
            last_log2 = now

    cap.release()
    set_status(vid, "done")
    logging.info("emitted %d/%d snapshots for VID=%s", emit_count, total_ready, vid)

def _hb_loop(stop_evt: threading.Event, rh: str):
    # ChangeMessageVisibility heartbeat while processing
    while not stop_evt.wait(timeout=max(1, VIS_HEARTBEAT_SEC)):
        try:
            sqs.change_message_visibility(
                QueueUrl=SQS_VIDEO_QUEUE_URL,
                ReceiptHandle=rh,
                VisibilityTimeout=VIS_TIMEOUT
            )
            logging.info("[heartbeat] extended visibility to %ss", VIS_TIMEOUT)
        except Exception as e:
            logging.warning("[heartbeat] failed: %s", e)

def run():
    logging.info("polling: %s at %s", SQS_VIDEO_QUEUE_URL, utcnow_iso())
    while True:
        rs = sqs.receive_message(
            QueueUrl=SQS_VIDEO_QUEUE_URL,
            MaxNumberOfMessages=1,
            WaitTimeSeconds=20,
            VisibilityTimeout=VIS_TIMEOUT if VIS_HEARTBEAT_SEC <= 0 else max(VIS_TIMEOUT, 120)
        )
        for m in rs.get("Messages", []):
            rh = m["ReceiptHandle"]
            payload = None
            stop_evt = threading.Event()
            hb_thread = None
            try:
                body = json.loads(m["Body"])
                payload = body if isinstance(body, dict) else json.loads(body)
                if payload.get("event") != "PROCESS_VIDEO":
                    sqs.delete_message(QueueUrl=SQS_VIDEO_QUEUE_URL, ReceiptHandle=rh)
                    continue
                # start heartbeat if enabled
                if VIS_HEARTBEAT_SEC > 0:
                    hb_thread = threading.Thread(target=_hb_loop, args=(stop_evt, rh), daemon=True)
                    hb_thread.start()
                handle_process_video(payload)
                sqs.delete_message(QueueUrl=SQS_VIDEO_QUEUE_URL, ReceiptHandle=rh)
            except KeyboardInterrupt:
                raise
            except Exception as e:
                err = f"{type(e).__name__}: {e}"
                logging.error("ERROR: %s", err)
                logging.error(traceback.format_exc())
                if payload and isinstance(payload, dict):
                    set_status(payload.get("video_id","00000000-0000-0000-0000-000000000000"), "error", err=err)
                # leave for retry/DLQ
            finally:
                if hb_thread:
                    stop_evt.set()
                    hb_thread.join(timeout=3)

if __name__ == "__main__":
    try:
        run()
    except KeyboardInterrupt:
        print("bye")
